{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n8lpzzFC9VD",
        "outputId": "cc912f20-20fd-44aa-c035-c90da59d6e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the pretrained GLoVe embeddings (https://nlp.stanford.edu/projects/glove/)\n",
        "# You can download and unzip the following archive: http://nlp.stanford.edu/data/glove.6B.zip\n",
        "import os.path\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "GLOVE_DIR = \"\"\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), \"r\")\n",
        "for line in f:\n",
        " values = line.split()\n",
        " word = values[0]\n",
        " coefs = np.asarray(values[1:], dtype='float32')\n",
        " embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('In total, there are %s word vectors.' % len(embeddings_index))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In total, there are 8564 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwQ9ETKY5vVg"
      },
      "source": [
        "Remove characters innecesaries of the questions list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpN5zS3gx33I"
      },
      "source": [
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('@','',text)\n",
        "    text = re.sub('_','',text)\n",
        "    text = re.sub(\"'\",\"\",text)\n",
        "    text = re.sub('\"',\"\",text)\n",
        "    text = re.sub('-',' ',text)\n",
        "    text = re.sub(':','',text)\n",
        "    text = re.sub('/','',text)\n",
        "    text = re.sub('<','',text)\n",
        "    text = re.sub('>','',text)\n",
        "    text = re.sub('!','',text)\n",
        "    text = re.sub('\\?','',text)\n",
        "    text = re.sub('/','',text)\n",
        "    text = re.sub('\\.','',text)\n",
        "    text = re.sub('\\.','',text)\n",
        "    text = re.sub('[0,9]','',text)\n",
        "    text = re.sub('\\'s','',text)\n",
        "    \n",
        "    \n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRtchh_7uWz"
      },
      "source": [
        "Calculate the most similars sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-LQUySuAjr"
      },
      "source": [
        "-------------Function Cosine Similiraty Token-------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5a-rNkQT7Wm"
      },
      "source": [
        "def cosine_similarity_token(a, b):\n",
        "    nominator = np.dot(a, b)\n",
        "    \n",
        "    a_norm = np.sqrt(np.sum(a**2))\n",
        "    b_norm = np.sqrt(np.sum(b**2))\n",
        "    \n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgywPI3V1wfx"
      },
      "source": [
        "-------------Function Cosine Similarity Sentence------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH0Cl-1HV25u"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity_sentence(a,b):\n",
        "\n",
        "    a = clean(a);\n",
        "    b = clean(b);\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "\n",
        "    sum_a = np.zeros(100)\n",
        "    for element in values_a:\n",
        "      if element in embeddings_index:\n",
        "        sum_a += embeddings_index[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        " \n",
        "    average_a = sum_a / len(values_a)\n",
        "\n",
        "    sum_b = np.zeros(100)\n",
        "    for element in values_b:\n",
        "      if element in embeddings_index:\n",
        "        sum_b += embeddings_index[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi__Dkv4T-BA",
        "outputId": "2343b1be-416a-43b0-e883-4b8c012c6b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cosine_similarity_token(embeddings_index['desk'], embeddings_index['table'])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4992076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAW-a-_L7z36"
      },
      "source": [
        "last_max_cosine = 0\n",
        "\n",
        "top_similar = list()\n",
        "\n",
        "#Calculate the most similars\n",
        "for i in range(len(questions_cleaned)):\n",
        "  if len(top_similar) < 10 and cosine_similarity_sentence(questions_cleaned[i], questions_cleaned[i+1]) > last_max_cosine:\n",
        "      top_similar.append(questions_cleaned[i])\n",
        "      last_max_cosine = cosine_similarity_sentence(questions_cleaned[i], questions_cleaned[i+1])\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqMJxu5oZF5P",
        "outputId": "f84e632d-777d-4044-9169-c30ac9350d4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The cosine similarity is: \",cosine_similarity_sentence(\"I am dead\", \"Having a desk is boring\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cosine similarity is:  0.6916605936245704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTUF8ko1TxWO"
      },
      "source": [
        "#We list al the auxiliary verbs in order to exclude them from our verb selection\n",
        "aux_verbs = [\"be\",\"am\",\"are\",\"is\",\"was\",\"were\",\"being\", \"can\", \"could\", \"do\", \"did\", \"does\", \"doing\", \"have\", \"had\", \"has\", \"having\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"]\n",
        "\n",
        "def cosine_similarity_sentence_with_POS(a,b):\n",
        "\n",
        "    a = a.lower()\n",
        "    b = b.lower()\n",
        "\n",
        "    #We use split() and nltk.pos_tag to separate the sentence in words and transform each word into a tuple with\n",
        "    #each word and it0s corresponding gramatical category.\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    values_a = nltk.pos_tag(values_a)\n",
        "    values_b = nltk.pos_tag(values_b)\n",
        "\n",
        "\n",
        "    #We know that the length of embeddings_index of \"desk\" is 100 and we store this value.\n",
        "    #We also check that the word is a noun or a verb using the tags\n",
        "    sum_a = np.zeros(len(embeddings_index['desk']))\n",
        "    for element in values_a:\n",
        "      if element[0] in embeddings_index:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_a += embeddings_index[element[0]]\n",
        "    average_a = sum_a / len(values_a)\n",
        "    \n",
        "    #We do the same for the second sentence\n",
        "    sum_b = np.zeros(len(embeddings_index['desk']))\n",
        "    for element in values_b:\n",
        "      if element[0] in embeddings_index:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_b += embeddings_index[element[0]]\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    #we construct the nominator of the cosine similarity function\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    #we construct the denominator of the cosine similarity function\n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-kecFuqH-37",
        "outputId": "e3481ecb-694c-47a3-d328-b65d2c5f40ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"The cosine similarity is: \",cosine_similarity_sentence_with_POS(\"I have a table that is red\", \"Having a desk become boring\"))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "The cosine similarity is:  0.5404111862926111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlYsdjBMB5l",
        "outputId": "a9fadfc6-7ef1-48c9-e7e8-e40851bf4ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "\n",
        "top_similar = []\n",
        "\n",
        "top_pairs = []\n",
        "\n",
        "last_max_cosine = 0.9\n",
        "import json\n",
        "with open('dev-v2.0.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "for element in data['data']:\n",
        "  for qas in element['paragraphs']:\n",
        "    for question in qas['qas']:\n",
        "      q1 = question['question']\n",
        "      for element in re.split(',|\\.',qas['context']):\n",
        "\n",
        "          q2 = element\n",
        "\n",
        "          if q1 != q2:\n",
        "            if cosine_similarity_sentence(q1,q2) > last_max_cosine and q1 not in top_pairs:\n",
        "              top_pairs.append([q1,q2,cosine_similarity_sentence(q1,q2)])\n",
        "\n",
        "\n",
        "import operator\n",
        "\n",
        "\n",
        "top_pairs.sort(key = operator.itemgetter(2), reverse = True)\n",
        "\n",
        "\n",
        "display(top_pairs[0:10])\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[['When would a program not be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress?',\n",
              "  ' the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress',\n",
              "  0.9987318612702049],\n",
              " ['What compiles and reports on data about the size of design and construction companies?',\n",
              "  ' ENR compiles and reports on data about the size of design and construction companies',\n",
              "  0.9980851271323278],\n",
              " ['What is not used for a precise definition of what it means to solve a problem using a given amount of time and space?',\n",
              "  'For a precise definition of what it means to solve a problem using a given amount of time and space',\n",
              "  0.9979406167815413],\n",
              " ['When had the Brotherhood renounced violence as a means of achieving its goals?',\n",
              "  ' the Brotherhood had renounced violence as a means of achieving its goals',\n",
              "  0.9974885484685991],\n",
              " ['What continued southward and is also considered the first Rhine river?',\n",
              "  ' that continued northward and is considered the first Rhine river',\n",
              "  0.9974702941677397],\n",
              " ['What can not be achieved by ensuring different representations can transformed into each other efficiently?',\n",
              "  ' This can be achieved by ensuring that different representations can be transformed into each other efficiently',\n",
              "  0.9974113680572727],\n",
              " ['What states that only problems that cannot be solved in polynomial time can be feasibly computed on some computational device?',\n",
              "  ' the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device',\n",
              "  0.9973972200388461],\n",
              " ['What is the force exerted by standard gravity on one ton of mass?',\n",
              "  ' is the force exerted by standard gravity on one kilogram of mass',\n",
              "  0.9970923420550433],\n",
              " [\"Which theory states that slow geological processes are still occurring today, and have occurred throughout Earth's history?\",\n",
              "  \" This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today\",\n",
              "  0.9968514362603538],\n",
              " ['Focus on what is to ameliorate the many problems that arise from the often highly competitive and adversarial practices within the construction industry.',\n",
              "  ' The focus on co-operation is to ameliorate the many problems that arise from the often highly competitive and adversarial practices within the construction industry',\n",
              "  0.9967167828959259]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-x_OuMrkoxX",
        "outputId": "643f6e5d-7f92-4a4b-bf09-8a58c0928d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "top_similar = []\n",
        "last_max_cosine = 0\n",
        "tokens = []\n",
        "import json\n",
        "with open('dev-v2.0.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "for element in data['data']:\n",
        "  for qas in element['paragraphs']:\n",
        "    for question in qas['qas']:\n",
        "      q1 = clean(question['question']).split()\n",
        "      for word in q1:\n",
        "        if word not in tokens:\n",
        "          tokens.append(word)\n",
        "\n",
        "for element in tokens:\n",
        "  token1 = element\n",
        "  for element2 in tokens:\n",
        "    token2 = element2\n",
        "    if token1 != token2 and token1 in embeddings_index and token2 in embeddings_index:\n",
        "      if len(top_similar) < 10 and cosine_similarity_token(embeddings_index[token1],embeddings_index[token2]) > last_max_cosine:\n",
        "        top_similar.append(token2)\n",
        "print(top_similar)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', 'country', 'is', 'located', 'when', 'were', 'the', 'from', 'which', 'countries']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW-Ly15_eE6B"
      },
      "source": [
        "Comparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUPeOoIk_F-o"
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#The path of our shared drive in the folder named Proc Lleng Natural\n",
        "data_path = '/content/drive/Shared drives/Proc Lleng Natural/Lab1/dataset/'\n",
        "\n",
        "!ls '/content/drive/Shared drives/Proc Lleng Natural/Lab1/dataset'\n",
        "\n",
        "#Load the dataset \n",
        "train_data = data_path + 'wiki-news-300d-1M-subword.vec'\n",
        "\n",
        "word_vectors = {}\n",
        "with open(train_data, 'r') as fin:\n",
        "  for line in fin:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        word_vectors[word] = coefs\n",
        "del word_vectors[\"999994\"]\n",
        "print('In total, there are %s word vectors.' % len(word_vectors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lf6Uvyok6DS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity_sentence(a,b):\n",
        "\n",
        "    a = clean(a);\n",
        "    b = clean(b);\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "\n",
        "    sum_a = np.zeros(len(word_vectors['desk']))\n",
        "    for element in values_a:\n",
        "      if element in word_vectors:\n",
        "        sum_a += word_vectors[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        " \n",
        "    average_a = sum_a / len(values_a)\n",
        "\n",
        "    sum_b = np.zeros(len(word_vectors['desk']))\n",
        "    for element in values_b:\n",
        "      if element in word_vectors:\n",
        "        sum_b += word_vectors[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tg44qW4e9rP"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#We list al the auxiliary verbs in order to exclude them from our verb selection\n",
        "aux_verbs = [\"be\",\"am\",\"are\",\"is\",\"was\",\"were\",\"being\", \"can\", \"could\", \"do\", \"did\", \"does\", \"doing\", \"have\", \"had\", \"has\", \"having\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"]\n",
        "\n",
        "def cosine_similarity_sentence_with_POS(a,b):\n",
        "\n",
        "    a = a.lower()\n",
        "    b = b.lower()\n",
        "\n",
        "    #We use split() and nltk.pos_tag to separate the sentence in words and transform each word into a tuple with\n",
        "    #each word and it0s corresponding gramatical category.\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    values_a = nltk.pos_tag(values_a)\n",
        "    values_b = nltk.pos_tag(values_b)\n",
        "\n",
        "\n",
        "    #We know that the length of embeddings_index of \"desk\" is 100 and we store this value.\n",
        "    #We also check that the word is a noun or a verb using the tags\n",
        "    sum_a = np.zeros(len(word_vectors['desk']))\n",
        "    for element in values_a:\n",
        "      if element[0] in word_vectors:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_a += word_vectors[element[0]]\n",
        "    average_a = sum_a / len(values_a)\n",
        "    \n",
        "    #We do the same for the second sentence\n",
        "    sum_b = np.zeros(len(word_vectors['desk']))\n",
        "    for element in values_b:\n",
        "      if element[0] in word_vectors:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_b += word_vectors[element[0]]\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    #we construct the nominator of the cosine similarity function\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    #we construct the denominator of the cosine similarity function\n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity\n",
        "\n",
        "print(\"The cosine similarity token is: \",cosine_similarity_token(word_vectors['desk'], word_vectors['table']))\n",
        "print(\"The cosine similarity is: \",cosine_similarity_sentence(\"I am dead\", \"Having a desk is boring\"))\n",
        "print(\"The cosine similarity with POS is: \",cosine_similarity_sentence_with_POS(\"I have a table that is red\", \"Having a desk become boring\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9toKl4WfZi25",
        "outputId": "e4c0b1e2-fc55-4009-b6b1-d6ec57d681bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#The path of our shared drive in the folder named Proc Lleng Natural\n",
        "data_path = '/content/drive/Shared drives/Proc Lleng Natural/Lab1/dataset/'\n",
        "\n",
        "!ls '/content/drive/Shared drives/Proc Lleng Natural/Lab1/dataset'\n",
        "\n",
        "#Load the dataset \n",
        "train_data = data_path + 'GoogleNews-vectors-negative300.vec'\n",
        "\n",
        "word_vectors2 = {}\n",
        "with open(train_data, 'r') as fin:\n",
        "  for line in fin:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        word_vectors2[word] = coefs\n",
        "print('In total, there are %s word vectors.' % len(word_vectors2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "GoogleNews-vectors-negative300.vec     wiki-news-300d-1M-subword.vec\n",
            "GoogleNews-vectors-negative300.vec.gz\n",
            "In total, there are 3000000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzxYm00ZuQ4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity_sentence(a,b):\n",
        "\n",
        "    a = clean(a);\n",
        "    b = clean(b);\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "\n",
        "    sum_a = np.zeros(len(word_vectors2['desk']))\n",
        "    for element in values_a:\n",
        "      if element in word_vectors2:\n",
        "        sum_a += word_vectors2[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        " \n",
        "    average_a = sum_a / len(values_a)\n",
        "\n",
        "    sum_b = np.zeros(len(word_vectors2['desk']))\n",
        "    for element in values_b:\n",
        "      if element in word_vectors2:\n",
        "        sum_b += word_vectors2[element]\n",
        "      else:\n",
        "        sum_a = sum_a\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5HBrhJaEZI",
        "outputId": "40f0881c-fba9-4970-fd7e-fe81f49cf082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#We list al the auxiliary verbs in order to exclude them from our verb selection\n",
        "aux_verbs = [\"be\",\"am\",\"are\",\"is\",\"was\",\"were\",\"being\", \"can\", \"could\", \"do\", \"did\", \"does\", \"doing\", \"have\", \"had\", \"has\", \"having\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"]\n",
        "\n",
        "def cosine_similarity_sentence_with_POS(a,b):\n",
        "\n",
        "    a = a.lower()\n",
        "    b = b.lower()\n",
        "\n",
        "    #We use split() and nltk.pos_tag to separate the sentence in words and transform each word into a tuple with\n",
        "    #each word and it0s corresponding gramatical category.\n",
        "    values_a = a.split()\n",
        "    values_b = b.split()\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    values_a = nltk.pos_tag(values_a)\n",
        "    values_b = nltk.pos_tag(values_b)\n",
        "\n",
        "\n",
        "    #We know that the length of embeddings_index of \"desk\" is 100 and we store this value.\n",
        "    #We also check that the word is a noun or a verb using the tags\n",
        "    sum_a = np.zeros(len(word_vectors2['desk']))\n",
        "    for element in values_a:\n",
        "      if element[0] in word_vectors2:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_a += word_vectors2[element[0]]\n",
        "    average_a = sum_a / len(values_a)\n",
        "    \n",
        "    #We do the same for the second sentence\n",
        "    sum_b = np.zeros(len(word_vectors2['desk']))\n",
        "    for element in values_b:\n",
        "      if element[0] in word_vectors2:\n",
        "        if element[0] not in aux_verbs:\n",
        "          if element[1] == \"NN\" or \"V\" in element[1]:\n",
        "            sum_b += word_vectors2[element[0]]\n",
        "    average_b = sum_b / len(values_b)\n",
        "\n",
        "    #we construct the nominator of the cosine similarity function\n",
        "    nominator = np.dot(average_a, average_b)\n",
        "\n",
        "    a_norm = np.sqrt(np.sum(average_a**2))\n",
        "    b_norm = np.sqrt(np.sum(average_b**2))\n",
        "    \n",
        "    #we construct the denominator of the cosine similarity function\n",
        "    denominator = a_norm * b_norm\n",
        "    \n",
        "    cosine_similarity = nominator / denominator\n",
        "    \n",
        "    return cosine_similarity\n",
        "\n",
        "print(\"The cosine similarity token is: \",cosine_similarity_token(word_vectors2['desk'], word_vectors2['table']))\n",
        "print(\"The cosine similarity is: \",cosine_similarity_sentence(\"I am dead\", \"Having a desk is boring\"))\n",
        "print(\"The cosine similarity with POS is: \",cosine_similarity_sentence_with_POS(\"I have a table that is red\", \"Having a desk become boring\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cosine similarity token is:  0.4045672\n",
            "The cosine similarity is:  0.3065082589603107\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "The cosine similarity with POS is:  0.3179761596730724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtCisZAt2dM"
      },
      "source": [
        "# Nueva sección"
      ]
    }
  ]
}